# Data Mining Projects ‚Äì Portfolio Showcase

This repository features my hands-on work for a university-level **Data Mining** course. It includes end-to-end projects on classification, clustering, association rule mining, web scraping, and NLP. All work was completed using Python and Jupyter Notebooks.

---

## üí° Highlights

- üß† **ML Techniques:** Decision Trees, Random Forest, Logistic Regression, SVM, Hierarchical Clustering, Apriori, FP-Growth  
- üóÉ **Data Sources:** Titanic dataset, Reddit depression posts, student performance data, scraped web content  
- ‚úçÔ∏è **Text Mining & NLP:** TF-IDF, stopword removal, word clouds, topic classification  
- üåê **Web Scraping:** Extracted data using `requests` and `BeautifulSoup`  
- üìä **Tools & Libraries:** `pandas`, `scikit-learn`, `nltk`, `matplotlib`, `apyori`, `mlxtend`, `BeautifulSoup`

---

## Featured Project: Depression Content Detection

- Built a model to detect if a Reddit post is **about depression**
- Used TF-IDF vectorization and classifiers (LogReg, SVM)
- Delivered as a notebook, PDF report, and presentation

---


## Skills Demonstrated

- Data preprocessing & cleaning  
- Supervised & unsupervised machine learning  
- Text classification  
- Association rule mining  
- Visualization & reporting

---


## Course Topics Covered

| Week | Topic |
|------|-------|
| Week 6 | Decision Tree Implementation |
| Week 7 | Titanic Survival Prediction (Random Forest) |
| Week 9 | Association Rule Analysis using Apriori & FP-Growth |
| Week 10 | Midterm Project ‚Äì Student Performance Prediction |
| Week 11 | Hierarchical Modeling |
| Week 13 | Text Mining |
| Week 15 | Web Scraping |
| Final Project | Depression Content Detection using Reddit Posts |

---

## Techniques & Tools Used

- **Machine Learning Algorithms:**  
  Decision Tree, Random Forest, Logistic Regression, SVM, Naive Bayes, Clustering, Association Rule Mining (Apriori, FP-Growth)

- **Text & Web Data Processing:**  
  TF-IDF, NLP preprocessing, word clouds, scraping with `BeautifulSoup`

- **Libraries & Tools:**  
  `pandas`, `scikit-learn`, `matplotlib`, `seaborn`, `nltk`, `apyori`, `mlxtend`, `requests`, `BeautifulSoup`, `Jupyter Notebooks`

---

## Folder Structure

Each week's folder contains:
- Jupyter notebooks with complete implementation
- Input datasets (CSV, TXT, etc.)
- Additional assets such as plots, screenshots, or output files
- A `README.md` for that week explaining the task and method

The **Final Project** folder includes:
- Cleaned dataset
- Complete modeling notebook
- Report and presentation files
- Markdown README

---

## How to Run

1. Clone the repository:

```bash
git clone https://github.com/CheenoOng09/Data_Mining_Projects.git
```

2. Navigate to a week's folder and install dependencies:

```bash
pip install -r requirements.txt  # if present
```

3. Launch Jupyter Notebook:

```bash
jupyter notebook
```

4. Open the `.ipynb` file and run the cells.

---

## Final Project Highlight

**Topic:** Depression Content Detection  
**Goal:** Classify whether Reddit posts are about depression using NLP and machine learning.  
**Techniques:** TF-IDF, Logistic Regression, SVM, Data Visualization  
**Deliverables:** Notebook, Dataset, Report, and Presentation

---

## License

This repository is intended for academic learning and showcase purposes only.

---
